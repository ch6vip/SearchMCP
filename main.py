import httpx
import asyncio
import sqlite3
import re
import urllib.parse
import os
from datetime import datetime
from fastmcp import FastMCP
from camoufox.async_api import AsyncCamoufox
from starlette.responses import HTMLResponse, JSONResponse
from markdownify import markdownify as md

# å¿½ç•¥çƒ¦äººçš„ Pydantic è­¦å‘Š
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

# å®šä¹‰ MCP æœåŠ¡
mcp = FastMCP("Web Surfer")
SEARXNG_URL = "http://127.0.0.1:10003"

# æ•°æ®åº“æ–‡ä»¶è·¯å¾„
DB_PATH = os.getenv("DB_PATH", "/app/usage_stats.db")

# --- æ•°æ®åº“åˆå§‹åŒ– ---
def init_db():
    # ç¡®ä¿æ•°æ®åº“ç›®å½•å­˜åœ¨
    db_dir = os.path.dirname(DB_PATH)
    if db_dir and not os.path.exists(db_dir):
        os.makedirs(db_dir, exist_ok=True)
    
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS usage_log (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            tool_name TEXT NOT NULL,
            timestamp TEXT NOT NULL
        )
    """)
    conn.commit()
    conn.close()

def log_usage(tool_name: str):
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute("INSERT INTO usage_log (tool_name, timestamp) VALUES (?, ?)",
                   (tool_name, datetime.now().isoformat()))
    conn.commit()
    conn.close()

init_db()

# --- å†…å®¹ç¼“å­˜ç®¡ç† ---
_content_cache = {}

# --- å…¨å±€ Camoufox æµè§ˆå™¨å•ä¾‹ç®¡ç† ---
_global_browser = None

async def get_browser():
    """
    æ‡’åŠ è½½ï¼šåªåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶å¯åŠ¨ Camoufox æµè§ˆå™¨ã€‚
    ä½¿ç”¨ Camoufox åæ£€æµ‹æµè§ˆå™¨ï¼Œèƒ½ç»•è¿‡ Cloudflare ç­‰ WAFã€‚
    """
    global _global_browser
    if _global_browser is None:
        print("ğŸš€ æ­£åœ¨åˆå§‹åŒ– Camoufox æµè§ˆå™¨å†…æ ¸ (ä»…éœ€ä¸€æ¬¡)...")
        # å¯åŠ¨ Camoufox åæ£€æµ‹æµè§ˆå™¨
        # ä½¿ç”¨è™šæ‹Ÿæ˜¾ç¤ºå™¨æ¨¡å¼è€Œé headlessï¼Œæ›´éš¾è¢«æ£€æµ‹
        
        # åˆ¤æ–­æ“ä½œç³»ç»Ÿï¼Œå¦‚æœæ˜¯ Windows åˆ™ä¸èƒ½ä½¿ç”¨ virtual æ¨¡å¼
        import platform
        is_windows = platform.system() == "Windows"
        headless_mode = True if is_windows else "virtual"
        
        _global_browser = await AsyncCamoufox(
            headless=headless_mode,  # Windows ä¸‹å›é€€åˆ°æ™®é€š headless
            # geoip=True,  # æ ¹æ® IP è‡ªåŠ¨è®¾ç½®åœ°ç†ä½ç½®
        ).__aenter__()
        print("âœ… Camoufox æµè§ˆå™¨å†…æ ¸å·²å°±ç»ª")
    return _global_browser


async def cleanup_browser():
    """æ¸…ç†æµè§ˆå™¨èµ„æº"""
    global _global_browser
    if _global_browser:
        await _global_browser.__aexit__(None, None, None)
        _global_browser = None
        # ç»™ä¸€ç‚¹æ—¶é—´è®©åº•å±‚è¿›ç¨‹å®Œå…¨é€€å‡ºï¼Œå‡å°‘ Windows ä¸Šçš„ pipe å…³é—­æŠ¥é”™å™ªéŸ³
        await asyncio.sleep(0.5)

# --- è¾…åŠ©å‡½æ•°ï¼šæ ¼å¼åŒ– SearXNG æ•°æ® ---
def format_searx_extras(data: dict) -> str:
    parts = []
    # 1. ç›´æ¥å›ç­”
    if "answers" in data and data["answers"]:
        parts.append("### ğŸ’¡ ç›´æ¥å›ç­”")
        for ans in data["answers"]:
            parts.append(f"- {ans}")
        parts.append("")
    # 2. çŸ¥è¯†å¡ç‰‡
    if "infoboxes" in data and data["infoboxes"]:
        for box in data["infoboxes"]:
            box_title = box.get("infobox", "Info")
            content = box.get("content", "")
            parts.append(f"### â„¹ï¸ çŸ¥è¯†å¡ç‰‡ ({box_title})")
            if content:
                parts.append(f"**æ‘˜è¦**: {content}")
            if "attributes" in box and box["attributes"]:
                parts.append("| å±æ€§ | å€¼ |")
                parts.append("| --- | --- |")
                for attr in box["attributes"]:
                    label = attr.get("label", "")
                    value = attr.get("value", "")
                    if label and value:
                        parts.append(f"| {label} | {value} |")
            if "urls" in box and box["urls"]:
                links = [f"[{u.get('title', 'Link')}]({u.get('url', '')})" for u in box["urls"]]
                parts.append(f"**ç›¸å…³é“¾æ¥**: {', '.join(links)}")
            parts.append("")
    return "\n".join(parts)

# --- è¾…åŠ©å‡½æ•°ï¼šæ‰§è¡Œå•æ¬¡æœç´¢ ---
async def _do_single_search(client: httpx.AsyncClient, query: str) -> dict:
    """
    æ‰§è¡Œå•æ¬¡æœç´¢è¯·æ±‚ï¼Œè¿”å›åŸå§‹ JSON æ•°æ®
    """
    search_url = f"{SEARXNG_URL}/search"
    params = {"q": query, "format": "json", "language": "zh-CN"}
    
    try:
        response = await client.get(search_url, params=params)
        response.raise_for_status()
        return response.json()
    except Exception:
        return {"results": [], "answers": [], "infoboxes": [], "suggestions": []}


# --- å·¥å…· 1: æœç´¢ ---
@mcp.tool()
async def web_search(query: str, limit: int = 5) -> str:
    """
    æœç´¢äº’è”ç½‘ã€‚åŒ…å«æœç´¢ç»“æœåˆ—è¡¨ã€çŸ¥è¯†å¡ç‰‡(Infobox)å’Œç›¸å…³å»ºè®®ã€‚
    
    æœç´¢ç­–ç•¥ï¼š
    1. é¦–å…ˆæœç´¢å®Œæ•´å…³é”®è¯
    2. å°†å…³é”®è¯æŒ‰ç©ºæ ¼æ‹†åˆ†ï¼Œå¯¹æ¯ä¸ªè¯å•ç‹¬æœç´¢
    3. æ•´åˆæ‰€æœ‰ç»“æœå¹¶å»é‡
    """
    log_usage("web_search")
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "application/json"
    }

    try:
        async with httpx.AsyncClient(timeout=10.0, headers=headers) as client:
            # æ”¶é›†æ‰€æœ‰æœç´¢ä»»åŠ¡
            search_queries = [query]  # å®Œæ•´å…³é”®è¯
            
            # æŒ‰ç©ºæ ¼æ‹†åˆ†å…³é”®è¯
            keywords = query.strip().split()
            if len(keywords) > 1:
                # æ·»åŠ æ¯ä¸ªå•ç‹¬çš„å…³é”®è¯ï¼ˆå»é‡ï¼‰
                for kw in keywords:
                    kw = kw.strip()
                    if kw and kw not in search_queries:
                        search_queries.append(kw)
            
            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æœç´¢
            tasks = [_do_single_search(client, q) for q in search_queries]
            all_data = await asyncio.gather(*tasks)
            
            # æ•´åˆç»“æœ
            merged_results = []
            seen_urls = set()  # ç”¨äºå»é‡
            all_answers = []
            all_infoboxes = []
            all_suggestions = set()
            
            for i, data in enumerate(all_data):
                query_label = search_queries[i]
                
                # æ”¶é›† answers (å¤„ç†å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–å­—å…¸çš„æƒ…å†µ)
                if "answers" in data and data["answers"]:
                    for ans in data["answers"]:
                        if isinstance(ans, dict):
                            ans_str = ans.get("answer", str(ans))
                        else:
                            ans_str = str(ans)
                        if ans_str not in all_answers:
                            all_answers.append(ans_str)
                
                # æ”¶é›† infoboxes (é€šè¿‡ infobox æ ‡é¢˜å»é‡)
                if "infoboxes" in data and data["infoboxes"]:
                    for box in data["infoboxes"]:
                        box_id = box.get("infobox", "") or box.get("id", str(box))
                        if not any(b.get("infobox", "") == box_id for b in all_infoboxes):
                            all_infoboxes.append(box)
                
                # æ”¶é›† suggestions (å¤„ç†å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–å­—å…¸çš„æƒ…å†µ)
                if "suggestions" in data and data["suggestions"]:
                    for sug in data["suggestions"]:
                        if isinstance(sug, dict):
                            sug_str = sug.get("suggestion", str(sug))
                        else:
                            sug_str = str(sug)
                        all_suggestions.add(sug_str)
                
                # æ”¶é›†æœç´¢ç»“æœï¼ˆå»é‡ï¼‰
                results = data.get("results", [])
                for result in results:
                    url = result.get("url", "")
                    if url and url not in seen_urls:
                        seen_urls.add(url)
                        result["_source_query"] = query_label  # æ ‡è®°æ¥æº
                        merged_results.append(result)
            
            # æ„å»ºè¾“å‡º
            output_blocks = []
            
            # æ˜¾ç¤ºæœç´¢ç­–ç•¥
            if len(search_queries) > 1:
                output_blocks.append(f"### ğŸ” æœç´¢ç­–ç•¥\nå·²æœç´¢ {len(search_queries)} ä¸ªå…³é”®è¯: `{'`, `'.join(search_queries)}`")
            
            # ä¼˜å…ˆæ˜¾ç¤ºçŸ¥è¯†å¡ç‰‡
            merged_data = {
                "answers": all_answers,  # å·²åœ¨ä¸Šé¢å»é‡
                "infoboxes": all_infoboxes
            }
            extras = format_searx_extras(merged_data)
            if extras:
                output_blocks.append(extras)

            # æœç´¢ç»“æœ
            if merged_results:
                output_blocks.append(f"### ğŸ” æœç´¢ç»“æœ (å…± {len(merged_results)} æ¡ï¼Œæ˜¾ç¤ºå‰ {min(limit, len(merged_results))} æ¡)")
                for i, result in enumerate(merged_results[:limit], 1):
                    title = result.get("title", "No Title")
                    link = result.get("url", "#")
                    content = result.get("content", "No Content")
                    source = result.get("_source_query", "")
                    source_tag = f" `[{source}]`" if source != query else ""
                    output_blocks.append(f"{i}. **[{title}]({link})**{source_tag}\n   {content}")
            else:
                output_blocks.append("æœªæ‰¾åˆ°å¸¸è§„ç½‘é¡µç»“æœã€‚")

            # å»ºè®®
            if all_suggestions:
                sorted_suggestions = sorted(list(all_suggestions))[:10]  # é™åˆ¶æ•°é‡
                output_blocks.append(f"\n**ç›¸å…³æœç´¢å»ºè®®**: {', '.join(sorted_suggestions)}")

            return "\n\n".join(output_blocks)
    except Exception as e:
        return f"æœç´¢å‡ºé”™: {str(e)}"

async def _read_url_impl(url: str, page: int = 1, chunk_size: int = 15000) -> str:
    """
    è®¿é—®å¹¶æŠ“å–æŒ‡å®š URL çš„ç½‘é¡µå†…å®¹ï¼Œæ”¯æŒåˆ†é¡µæŸ¥çœ‹ã€‚
    ä½¿ç”¨ Camoufox åæ£€æµ‹æµè§ˆå™¨ï¼Œèƒ½ç»•è¿‡ Cloudflare ç­‰ WAFã€‚

    å‚æ•°:
    - url: è¦æŠ“å–çš„ç½‘å€
    - page: é¡µç ï¼ˆä»1å¼€å§‹ï¼‰
    - chunk_size: æ¯é¡µå­—ç¬¦æ•°ï¼ˆé»˜è®¤15000ï¼‰
    """
    log_usage("read_url")
    try:
        # æ£€æŸ¥ç¼“å­˜
        if url not in _content_cache:
            browser = await get_browser()
            print(f"ğŸ¦Š æ­£åœ¨ä½¿ç”¨ Camoufox æŠ“å–: {url}")

            # åˆ›å»ºæ–°é¡µé¢å¹¶æŠ“å–
            page_obj = await browser.new_page()
            try:
                await page_obj.goto(url, wait_until="networkidle", timeout=60000)
                # ç­‰å¾…é¡µé¢ç¨³å®š
                await asyncio.sleep(1)
                # è·å– HTML å†…å®¹å¹¶è½¬æ¢ä¸º Markdown
                html_content = await page_obj.content()
                markdown_content = md(html_content, heading_style="ATX", strip=['script', 'style'])
                _content_cache[url] = markdown_content
            finally:
                await page_obj.close()

        content = _content_cache[url]
        total_pages = (len(content) + chunk_size - 1) // chunk_size

        if total_pages == 0:
            return f"### ğŸ“„ é¡µé¢å†…å®¹: {url}\n\né¡µé¢å†…å®¹ä¸ºç©ºæˆ–æ— æ³•è§£æã€‚"

        if page < 1 or page > total_pages:
            return f"é¡µç æ— æ•ˆã€‚æ€»å…± {total_pages} é¡µï¼Œè¯·é€‰æ‹© 1-{total_pages}"

        start = (page - 1) * chunk_size
        end = min(start + chunk_size, len(content))
        chunk = content[start:end]

        header = f"### ğŸ“„ é¡µé¢å†…å®¹: {url}\n**ç¬¬ {page}/{total_pages} é¡µ** (å­—ç¬¦ {start+1}-{end}/{len(content)})\n\n"
        footer = f"\n\n---\nğŸ’¡ ä½¿ç”¨ `read_url(url=\"{url}\", page={page+1})` æŸ¥çœ‹ä¸‹ä¸€é¡µ" if page < total_pages else ""

        return header + chunk + footer

    except Exception as e:
        return f"æŠ“å–å¼‚å¸¸: {str(e)}"

# --- å·¥å…· 2: æŠ“å– ---
@mcp.tool()
async def read_url(url: str, page: int = 1, chunk_size: int = 15000) -> str:
    return await _read_url_impl(url, page, chunk_size)


async def _google_search_impl(query: str, limit: int = 10) -> str:
    """
    ä½¿ç”¨ Bing æœç´¢å¹¶è¿”å›ç»“æœã€‚é€šè¿‡ Camoufox åæ£€æµ‹æµè§ˆå™¨çˆ¬å– Bing æœç´¢é¡µé¢ã€‚

    å‚æ•°:
    - query: æœç´¢å…³é”®è¯
    - limit: è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤10æ¡ï¼‰
    """
    log_usage("google_search")

    try:
        browser = await get_browser()

        # æ„å»º Bing æœç´¢ URL (å›½å†…å¯ç”¨)
        search_url = f"https://www.bing.com/search?q={urllib.parse.quote(query)}"
        # æ³¨æ„ï¼šBing ä¸æ”¯æŒ num å‚æ•°ï¼Œlimit é€»è¾‘ä¸»è¦é åç»­çš„æ­£åˆ™æå–æ§åˆ¶

        print(f"ğŸ¦Š æ­£åœ¨ä½¿ç”¨ Camoufox æœç´¢ Bing: {query}")

        # åˆ›å»ºæ–°é¡µé¢å¹¶æŠ“å–
        page_obj = await browser.new_page()
        try:
            await page_obj.goto(search_url, wait_until="networkidle", timeout=60000)
            # ç­‰å¾…é¡µé¢ç¨³å®š
            await asyncio.sleep(2)

            # è·å– HTML å†…å®¹å¹¶è½¬æ¢ä¸º Markdown
            html_content = await page_obj.content()
            markdown_content = md(html_content, heading_style="ATX", strip=['script', 'style'])
        finally:
            await page_obj.close()

        # ä½¿ç”¨ç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼æå–æœç´¢ç»“æœ
        output_blocks = []
        output_blocks.append(f"### ğŸ” Bing æœç´¢ç»“æœ: `{query}`\n")

        # æå–æœç´¢ç»“æœé“¾æ¥å’Œæ ‡é¢˜
        results = []

        # æå– markdown ä¸­çš„é“¾æ¥ [title](url)
        link_pattern = r'\[([^\]]+)\]\((https?://[^\)]+)\)'
        matches = re.findall(link_pattern, markdown_content)

        seen_urls = set()
        for title, url in matches:
            # è¿‡æ»¤ Google å’Œ Bing è‡ªèº«çš„é“¾æ¥
            if 'google.com' in url or 'bing.com' in url or 'microsoft.com' in url:
                continue
            if url in seen_urls:
                continue
            if len(title.strip()) < 3:
                continue
            seen_urls.add(url)
            results.append({
                'title': title.strip(),
                'url': url
            })
            if len(results) >= limit:
                break

        if results:
            output_blocks.append(f"æ‰¾åˆ° {len(results)} æ¡ç»“æœ:\n")
            for i, r in enumerate(results, 1):
                output_blocks.append(f"{i}. **[{r['title']}]({r['url']})**")
        else:
            # å¦‚æœæ­£åˆ™æ²¡æœ‰æå–åˆ°ç»“æœï¼Œè¿”å›åŸå§‹ markdown å†…å®¹çš„æ‘˜è¦
            output_blocks.append("æœªèƒ½è§£æåˆ°ç»“æ„åŒ–ç»“æœï¼Œä»¥ä¸‹æ˜¯é¡µé¢å†…å®¹æ‘˜è¦:\n")
            # æˆªå–å‰ 5000 å­—ç¬¦
            summary = markdown_content[:5000] if len(markdown_content) > 5000 else markdown_content
            output_blocks.append(summary)

        return "\n\n".join(output_blocks)

    except Exception as e:
        return f"Bing æœç´¢å‡ºé”™: {str(e)}"


# --- å·¥å…· 3: è°·æ­Œæœç´¢ ---
@mcp.tool()
async def google_search(query: str, limit: int = 10) -> str:
    return await _google_search_impl(query, limit)


# --- Dashboard è·¯ç”± ---
from starlette.responses import FileResponse
import os

# --- é™æ€æ–‡ä»¶æœåŠ¡ ---
@mcp.custom_route("/static/{file_path:path}", methods=["GET"])
async def serve_static(request):
    file_path = request.path_params['file_path']
    full_path = os.path.join("static", file_path)
    if os.path.exists(full_path) and os.path.isfile(full_path):
        return FileResponse(full_path)
    return JSONResponse({"error": "File not found"}, status_code=404)

@mcp.custom_route("/dashboard", methods=["GET"])
async def dashboard(request):
    with open("templates/dashboard.html", "r", encoding="utf-8") as f:
        html = f.read()
    return HTMLResponse(html)

@mcp.custom_route("/api/stats", methods=["GET"])
async def api_stats(request):
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute("SELECT tool_name, COUNT(*) FROM usage_log GROUP BY tool_name")
    tool_stats = cursor.fetchall()
    cursor.execute("SELECT tool_name, timestamp FROM usage_log ORDER BY timestamp DESC LIMIT 20")
    recent_logs = cursor.fetchall()
    conn.close()
    return JSONResponse({"tool_stats": tool_stats, "recent_logs": recent_logs})

if __name__ == "__main__":
    # ä½¿ç”¨ SSE æ¨¡å¼è¿è¡Œ
    mcp.run(transport="sse", host="0.0.0.0", port=9191)
